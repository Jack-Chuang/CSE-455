{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jack-Chuang/UW-CSE-455/blob/main/Copy_of_keras_tuner_for_hyperparameters_and_model_layers_of_the_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34846a33",
      "metadata": {
        "id": "34846a33"
      },
      "source": [
        "#### import , load dataset, and image preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bayesian-optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P36JwTUuN_uj",
        "outputId": "ba924b34-ddbf-4931-949e-3f99e99f7b45"
      },
      "id": "P36JwTUuN_uj",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian-optimization-1.2.0.tar.gz (14 kB)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.21.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.1.0)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-py3-none-any.whl size=11685 sha256=f10db570b0f138724c36cdf9a41a0d61918dab668cd4c889be184b8a177bf335\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/9b/71/f127d694e02eb40bcf18c7ae9613b88a6be4470f57a8528c5b\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8d451412",
      "metadata": {
        "id": "8d451412",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea3f53ef-b1b7-458b-f50d-1e88b55653a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "170508288/170498071 [==============================] - 11s 0us/step\n",
            "[[6]\n",
            " [9]\n",
            " [9]\n",
            " ...\n",
            " [9]\n",
            " [1]\n",
            " [1]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, BatchNormalization, Dropout, Flatten, Conv2D, MaxPool2D, DepthwiseConv2D\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from math import floor\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "import pickle\n",
        "from keras.layers import LeakyReLU\n",
        "LeakyReLU = LeakyReLU(alpha=0.1)\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "# import test, train\n",
        "from keras.utils import np_utils\n",
        "import keras.backend as K\n",
        "\n",
        "# Set seed\n",
        "from numpy.random import seed\n",
        "seed(123)\n",
        "\n",
        "os.environ['PYTHONHASHSEED']=str(123) \n",
        "\n",
        "import random\n",
        "random.seed(123)\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(123)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
        "    \n",
        "x_train = X_train.astype('float32')/255\n",
        "x_test = X_test.astype('float32')/255\n",
        "y_train = tf.keras.utils.to_categorical(Y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(Y_test, 10)\n",
        "# y_train_multilabel = tf.keras.utils.to_categorical(Y_train, 10)\n",
        "# y_test_multilabel = tf.keras.utils.to_categorical(Y_test, 10)\n",
        "# y_train = LabelEncoder().fit_transform([''.join(str(l)) for l in y_train_multilabel])\n",
        "# y_test = LabelEncoder().fit_transform([''.join(str(l)) for l in y_test_multilabel])\n",
        "print(Y_train)\n",
        "# print(y_train_multilabel)\n",
        "print(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9181050e",
      "metadata": {
        "id": "9181050e"
      },
      "outputs": [],
      "source": [
        "# Make scorer accuracy\n",
        "score_acc = make_scorer(accuracy_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e58e1c1d",
      "metadata": {
        "id": "e58e1c1d"
      },
      "source": [
        "#### the function of building the baseline neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "946c13e1",
      "metadata": {
        "id": "946c13e1"
      },
      "outputs": [],
      "source": [
        "# Create function\n",
        "def nn_cl_bo2(cell_magnitude1, cell_magnitude2, conv_filter, conv_kernel, conv_activation, neurons, optimizer, \n",
        "              learning_rate, batch_size, epochs, dropout, dropout_rate):\n",
        "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
        "    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
        "                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
        "                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
        "                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
        "        \n",
        "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
        "                   'elu', 'exponential', LeakyReLU,'relu']\n",
        "     \n",
        "#     conv_activations = [\"relu\", \"selu\"]\n",
        "        \n",
        "    cell_magnitude1 = round(cell_magnitude1)\n",
        "    cell_magnitude2 = round(cell_magnitude2)\n",
        "    conv_filter = round(conv_filter)\n",
        "    conv_kernel = round(conv_kernel)\n",
        "    conv_activ = activationL[round(conv_activation)]\n",
        "    neurons = round(neurons)\n",
        "    optimizer = optimizerD[optimizerL[round(optimizer)]]\n",
        "    batch_size = round(batch_size)\n",
        "    epochs = round(epochs)\n",
        "    \n",
        "    def SepConv(nn):\n",
        "        nn.add(DepthwiseConv2D(kernel_size=conv_kernel,\n",
        "               padding='same',\n",
        "               depth_multiplier=1,\n",
        "               strides=1,\n",
        "               activation=conv_activ,\n",
        "               use_bias=False))\n",
        "        nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "        nn.add(layers.ReLU(6.))\n",
        "        nn.add(Conv2D(conv_filter, kernel_size=conv_kernel, padding='same', activation=conv_activ, use_bias=False, \n",
        "                    strides=1))\n",
        "        nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "        return nn\n",
        "    \n",
        "    def MBConv(nn):\n",
        "        nn.add(DepthwiseConv2D(kernel_size=conv_kernel,\n",
        "               padding='same',\n",
        "               depth_multiplier=1,\n",
        "               strides=1,\n",
        "               activation=conv_activ,\n",
        "               use_bias=False))\n",
        "        nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "        nn.add(layers.ReLU(6.))\n",
        "        nn.add(DepthwiseConv2D(kernel_size=conv_kernel,\n",
        "               padding='same',\n",
        "               depth_multiplier=1,\n",
        "               strides=1,\n",
        "               activation=conv_activ,\n",
        "               use_bias=False))\n",
        "        nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "        nn.add(layers.ReLU(6.))\n",
        "        nn.add(Conv2D(conv_filter, kernel_size=conv_kernel, padding='same', activation=conv_activ, use_bias=False, \n",
        "                    strides=1))\n",
        "        nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "        return nn\n",
        "        \n",
        "    def nn_cl_fun():\n",
        "        nn = Sequential()\n",
        "        nn.add(Conv2D(conv_filter, kernel_size=conv_kernel, input_shape=(32, 32, 3), strides=1, activation=conv_activ, padding='same'))\n",
        "        nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "        nn.add(layers.ReLU(6.))\n",
        "        nn = SepConv(nn)\n",
        "        for i in range(cell_magnitude1):\n",
        "            nn = MBConv(nn)\n",
        "        if dropout > 0.5:\n",
        "            nn.add(Dropout(dropout_rate, seed=123))\n",
        "        for i in range(cell_magnitude2):\n",
        "            nn = MBConv(nn)\n",
        "        nn.add(MaxPool2D(pool_size=(2, 2), padding='same', strides=1))\n",
        "        nn.add(Flatten())\n",
        "        nn.add(Dense(10, activation='softmax'))\n",
        "        nn.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=['accuracy'])\n",
        "        return nn\n",
        "        \n",
        "    model = nn_cl_fun() \n",
        "    model.predict(x_train)\n",
        "    trainableParams = np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])\n",
        "    nonTrainableParams = np.sum([np.prod(v.get_shape()) for v in model.non_trainable_weights])\n",
        "    totalParams = trainableParams + nonTrainableParams    \n",
        "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
        "    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "    \n",
        "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
        "    score = cross_val_score(nn, X_train, Y_train, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
        "    # score = cross_val_score(nn, x_train, y_train, scoring=score_acc, fit_params={'callbacks':[es]}).mean()\n",
        "    # final_score = accuracy * trainableParams * 1/10000\n",
        "    rounded_score = float(\"{0:.5f}\".format(score))\n",
        "    \n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def nn_cl_fun_try():\n",
        "    nn = Sequential()\n",
        "    nn.add(Conv2D(4, kernel_size=1, input_shape=(32, 32, 3), strides=1, activation='relu', padding='same'))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    nn.add(layers.ReLU(6.))\n",
        "    nn.add(DepthwiseConv2D(kernel_size=1,\n",
        "        padding='same',\n",
        "        depth_multiplier=1,\n",
        "        strides=1,\n",
        "        activation='relu',\n",
        "        use_bias=False))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    nn.add(layers.ReLU(6.))\n",
        "    nn.add(Conv2D(4, kernel_size=1, padding='same', activation='relu', use_bias=False, \n",
        "            strides=1))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "\n",
        "    nn.add(DepthwiseConv2D(kernel_size=1,\n",
        "            padding='same',\n",
        "            depth_multiplier=1,\n",
        "            strides=1,\n",
        "            activation='relu',\n",
        "            use_bias=False))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    nn.add(layers.ReLU(6.))\n",
        "    nn.add(DepthwiseConv2D(kernel_size=1,\n",
        "            padding='same',\n",
        "            depth_multiplier=1,\n",
        "            strides=1,\n",
        "            activation='relu',\n",
        "            use_bias=False))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    nn.add(layers.ReLU(6.))\n",
        "    nn.add(Conv2D(4, kernel_size=1, padding='same', activation='relu', use_bias=False, \n",
        "                strides=1))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "\n",
        "    nn.add(Dropout(0.2, seed=123))\n",
        "\n",
        "    nn.add(DepthwiseConv2D(kernel_size=1,\n",
        "            padding='same',\n",
        "            depth_multiplier=1,\n",
        "            strides=1,\n",
        "            activation='relu',\n",
        "            use_bias=False))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    nn.add(layers.ReLU(6.))\n",
        "    nn.add(DepthwiseConv2D(kernel_size=1,\n",
        "            padding='same',\n",
        "            depth_multiplier=1,\n",
        "            strides=1,\n",
        "            activation='relu',\n",
        "            use_bias=False))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    nn.add(layers.ReLU(6.))\n",
        "    nn.add(Conv2D(4, kernel_size=1, padding='same', activation='relu', use_bias=False, \n",
        "                strides=1))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "\n",
        "    nn.add(MaxPool2D(pool_size=(2, 2), padding='same', strides=1))\n",
        "    nn.add(Flatten())\n",
        "    nn.add(Dense(10, activation='softmax'))\n",
        "    nn.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return nn\n",
        "\n",
        "def nn_cl_fun2():\n",
        "    nn2 = Sequential()\n",
        "    nn2.add(Conv2D(filters=32, kernel_size=3, input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
        "    nn2.add(MaxPool2D(pool_size=2))\n",
        "    nn2.add(Conv2D(filters=32, kernel_size=3, input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
        "    nn2.add(MaxPool2D(pool_size=2))\n",
        "    nn2.add(Flatten())\n",
        "    nn2.add(Dense(10, activation='softmax'))\n",
        "    nn2.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return nn2\n",
        "\n",
        "# a = tf.keras.utils.to_categorical(np.random.randint(0,10,(50000,1)))\n",
        "nn = KerasClassifier(build_fn=nn_cl_fun, epochs=50, batch_size=500, verbose=0)\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
        "score = cross_val_score(nn, x_train, a, scoring=score_acc, cv=kfold).mean()\n",
        "# score = cross_val_score(nn, x_train, y_train, scoring=score_acc).mean()\n",
        "# nn2 = nn_cl_fun2()\n",
        "# history2 = nn2.fit(x_train, y_train, epochs=10, batch_size=64, verbose=1, validation_data=(x_test, y_test),shuffle=True)\n",
        "# print(history2.history['accuracy'])\n",
        "# loss, accuracy = model2.evaluate(x_train, y_train, verbose=0)\n",
        "# print('Accuracy: %f' % (accuracy))\n",
        "# print('Loss: %f' % (loss))\n",
        "# model2.fit(x_train, y_train, epochs=10, batch_size=64, verbose=1, validation_data=(x_test, y_test),shuffle=True)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "sqwJE7F0yqHa",
        "outputId": "f8f581ee-1a60-4f00-a6fe-6e529dc53db6"
      },
      "id": "sqwJE7F0yqHa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2a68876deb9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# a = tf.keras.utils.to_categorical(np.random.randint(0,10,(50000,1)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn_cl_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscore_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn_cl_fun' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = 13942.63765\n",
        "g = float(\"{0:.5f}\".format(x))\n",
        "print(g)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h8TecadtCL6",
        "outputId": "c78e8a3c-60d3-4a68-8109-d25b7016de32"
      },
      "id": "-h8TecadtCL6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13942.63765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer = tf.keras.layers.CategoryEncoding(\n",
        "           num_tokens=4, output_mode=\"one_hot\")\n",
        "layer([3.3, 1.99, 0.2, 1.1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0kEMkzj_PUe",
        "outputId": "ac768c5a-191d-48f2-b9f1-cb27bf6422ee"
      },
      "id": "W0kEMkzj_PUe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
              "array([[0., 0., 0., 1.],\n",
              "       [0., 1., 0., 0.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [0., 1., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices = [0, 2, -1, 1]\n",
        "depth = 3\n",
        "tf.one_hot(np.argmax(indices, axis=0), depth,\n",
        "           on_value=5.0, off_value=0.0,\n",
        "           axis=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd8La2ew_3Q6",
        "outputId": "cd6a02de-8e7b-4785-e8f0-8a837a2f35b7"
      },
      "id": "dd8La2ew_3Q6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 5., 0.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.keras.utils.to_categorical(np.random.randint(0,10,(5,1)) )\n",
        "print(np.random.randint(0,10,(5,1)) )\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM1ZS--2qx5_",
        "outputId": "1fdda83e-a201-4b49-a4f6-c850fa4cc714"
      },
      "id": "gM1ZS--2qx5_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[6]\n",
            " [5]\n",
            " [8]\n",
            " [8]\n",
            " [7]]\n",
            "[[0. 0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a70bae12",
      "metadata": {
        "id": "a70bae12"
      },
      "source": [
        "#### search for the best params using bayesian optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6704153",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "b6704153",
        "outputId": "05d539a0-b8cd-48d6-c810-fad0d66eb06c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | batch_... | cell_m... | cell_m... | conv_a... | conv_f... | conv_k... |  dropout  | dropou... |  epochs   | learni... |  neurons  | optimizer |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m nan     \u001b[0m | \u001b[0m 689.7   \u001b[0m | \u001b[0m 0.8453  \u001b[0m | \u001b[0m 2.18    \u001b[0m | \u001b[0m 6.923   \u001b[0m | \u001b[0m 8.725   \u001b[0m | \u001b[0m 1.597   \u001b[0m | \u001b[0m 0.02248 \u001b[0m | \u001b[0m 0.1261  \u001b[0m | \u001b[0m 39.09   \u001b[0m | \u001b[0m 0.3443  \u001b[0m | \u001b[0m 99.16   \u001b[0m | \u001b[0m 1.664   \u001b[0m |\n",
            "| \u001b[0m 2       \u001b[0m | \u001b[0m nan     \u001b[0m | \u001b[0m 265.0   \u001b[0m | \u001b[0m 3.348   \u001b[0m | \u001b[0m 3.106   \u001b[0m | \u001b[0m 2.468   \u001b[0m | \u001b[0m 11.46   \u001b[0m | \u001b[0m 1.473   \u001b[0m | \u001b[0m 0.07396 \u001b[0m | \u001b[0m 0.2702  \u001b[0m | \u001b[0m 83.52   \u001b[0m | \u001b[0m 0.8422  \u001b[0m | \u001b[0m 83.37   \u001b[0m | \u001b[0m 6.937   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: (661.8190636861435, 4.068834585182438, 2.106589386996094, 0.2470316383183836, 11.266186290535519, 1.4213043412603725, 0.8172200615549913, 0.20931832037868978, 65.22283210717812, 0.28148502249402685, 99.86263693206844, 0.9662943713915518)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-214e7f45a92d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Run Bayesian Optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mnn_bo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianOptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_cl_bo2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_nn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mnn_bo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-97e5dd85a6b0>\u001b[0m in \u001b[0;36mnn_cl_bo2\u001b[0;34m(cell_magnitude1, cell_magnitude2, conv_filter, conv_kernel, conv_activation, neurons, optimizer, learning_rate, batch_size, epochs, dropout, dropout_rate)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_cl_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mtrainableParams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mnonTrainableParams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_trainable_weights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-97e5dd85a6b0>\u001b[0m in \u001b[0;36mnn_cl_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSepConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell_magnitude1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMBConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdropout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-97e5dd85a6b0>\u001b[0m in \u001b[0;36mMBConv\u001b[0;34m(nn)\u001b[0m\n\u001b[1;32m     44\u001b[0m                \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconv_activ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                use_bias=False))\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         nn.add(DepthwiseConv2D(kernel_size=conv_kernel,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    218\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m-> 1033\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1172\u001b[0m       \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1174\u001b[0;31m           inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    938\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2740\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2741\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2742\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2743\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2744\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/normalization/batch_normalization.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    457\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariableAggregation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMEAN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m           experimental_autocast=False)\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m         caching_device=caching_device)\n\u001b[0m\u001b[1;32m    695\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m       \u001b[0;31m# TODO(fchollet): in the future, this should be handled at the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m       shape=variable_shape if variable_shape else None)\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m   def _variable_v2_call(cls,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m                         shape=None):\n\u001b[1;32m    202\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2721\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m   2722\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m     return variables.RefVariable(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1674\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m   def _init_from_args(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1924\u001b[0m           \u001b[0mis_initialized_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_initialized_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m           \u001b[0mcached_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcached_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1926\u001b[0;31m           caching_device=caching_device)\n\u001b[0m\u001b[1;32m   1927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1928\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_init_from_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, trainable, shape, dtype, handle, constraint, synchronization, aggregation, distribute_strategy, name, unique_id, handle_name, graph_element, initial_value, initializer_op, is_initialized_op, cached_value, save_slice_info, handle_deleter, caching_device, in_graph_mode, **unused_kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mhandle_deleter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         handle_deleter = EagerResourceDeleter(\n\u001b[0;32m--> 469\u001b[0;31m             handle=self._handle, handle_device=self._handle.device)\n\u001b[0m\u001b[1;32m    470\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_deleter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_shape_as_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# conv_input = [12]\n",
        "# conv_filters = [4, 8, 12, 16, 20]\n",
        "# conv_kernels = [1, 3, 5, 7]\n",
        "# conv_activations = [\"relu\", \"selu\"]\n",
        "# n_epochs = [20, 30, 40, 50]\n",
        "# n_batch = [1, 150]\n",
        "# n_diff = [0, 12]\n",
        "\n",
        "params_nn2 ={\n",
        "    'cell_magnitude1': (0, 5),\n",
        "    'cell_magnitude2': (0, 5),\n",
        "    'conv_filter': ((4, 20)),\n",
        "    'conv_kernel': ((1, 5)),\n",
        "    'conv_activation': ((0, 9)),\n",
        "    'neurons': (10, 100),\n",
        "    'optimizer':(0,7),\n",
        "    'learning_rate':(0.01, 1),\n",
        "    'batch_size':(200, 1000),\n",
        "    'epochs':(20, 100),\n",
        "    'dropout':(0,1),\n",
        "    'dropout_rate':(0,0.3)\n",
        "}\n",
        "\n",
        "# Run Bayesian Optimization\n",
        "nn_bo = BayesianOptimization(nn_cl_bo2, params_nn2, random_state=111)\n",
        "nn_bo.maximize(init_points=25, n_iter=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dac96728",
      "metadata": {
        "id": "dac96728"
      },
      "source": [
        "#### collect the best hyperparamters and number of layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ly2U3qenV9_0"
      },
      "id": "Ly2U3qenV9_0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d874520f",
      "metadata": {
        "id": "d874520f",
        "outputId": "cfc16b2d-fa91-453d-b943-3de9ee906e15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 285,\n",
              " 'cell_magnitude1': 2,\n",
              " 'cell_magnitude2': 4,\n",
              " 'conv_activation': 'sigmoid',\n",
              " 'conv_filter': 20,\n",
              " 'conv_kernel': 2,\n",
              " 'dropout': 0.7214115754924483,\n",
              " 'dropout_rate': 0.1982676082332063,\n",
              " 'epochs': 26,\n",
              " 'learning_rate': 0.7003114523771421,\n",
              " 'neurons': 34,\n",
              " 'optimizer': <keras.optimizer_v2.nadam.Nadam at 0x7efdb90bc750>}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "params_nn_ = nn_bo.max['params']\n",
        "\n",
        "learning_rate = params_nn_['learning_rate']\n",
        "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
        "               'elu', 'exponential', LeakyReLU,'relu']\n",
        "params_nn_['conv_activation'] = activationL[round(params_nn_['conv_activation'])]\n",
        "\n",
        "params_nn_['batch_size'] = round(params_nn_['batch_size'])\n",
        "params_nn_['epochs'] = round(params_nn_['epochs'])\n",
        "params_nn_['cell_magnitude1'] = round(params_nn_['cell_magnitude1'])\n",
        "params_nn_['cell_magnitude2'] = round(params_nn_['cell_magnitude2'])\n",
        "params_nn_['neurons'] = round(params_nn_['neurons'])\n",
        "params_nn_['conv_filter'] = round(params_nn_['conv_filter'])\n",
        "params_nn_['conv_kernel'] = round(params_nn_['conv_kernel'])\n",
        "params_nn_['conv_kernel'] = round(params_nn_['conv_kernel'])\n",
        "# params_nn_['dropout'] = round(params_nn_['dropout'])\n",
        "# params_nn_['dropout_rate'] = round(params_nn_['dropout_rate'])\n",
        "# params_nn_['learning_rate'] = round(params_nn_['learning_rate'])\n",
        "\n",
        "optimizerL = ['Adam', 'SGD', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','Adam']\n",
        "optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
        "             'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
        "             'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
        "             'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
        "params_nn_['optimizer'] = optimizerD[optimizerL[round(params_nn_['optimizer'])]]\n",
        "\n",
        "params_nn_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e176e5c",
      "metadata": {
        "id": "9e176e5c"
      },
      "source": [
        "#### use the collection of the best params to rebuild the model and try it on the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2347115c",
      "metadata": {
        "id": "2347115c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "757076fd-2300-44f0-ef98-bf699fa0a7ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 4ms/step - loss: 2.3969 - accuracy: 0.1000\n",
            "Accuracy: 0.100000\n",
            "Loss: 2.396929\n"
          ]
        }
      ],
      "source": [
        "# Fitting Neural Network\n",
        "def SepConv_final(nn):\n",
        "    nn.add(DepthwiseConv2D(kernel_size=params_nn_['conv_kernel'],\n",
        "            padding='same',\n",
        "            depth_multiplier=1,\n",
        "            strides=1,\n",
        "            activation=params_nn_['conv_activation'],\n",
        "            use_bias=False))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    nn.add(layers.ReLU(6.))\n",
        "    nn.add(Conv2D(params_nn_['conv_filter'], kernel_size=params_nn_['conv_kernel'], padding='same', activation=params_nn_['conv_activation'], use_bias=False, \n",
        "                strides=1))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    return nn\n",
        "\n",
        "def MBConv_final(nn):\n",
        "    nn.add(DepthwiseConv2D(kernel_size=params_nn_['conv_kernel'],\n",
        "            padding='same',\n",
        "            depth_multiplier=1,\n",
        "            strides=1,\n",
        "            activation=params_nn_['conv_activation'],\n",
        "            use_bias=False))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    nn.add(layers.ReLU(6.))\n",
        "    nn.add(DepthwiseConv2D(kernel_size=params_nn_['conv_kernel'],\n",
        "            padding='same',\n",
        "            depth_multiplier=1,\n",
        "            strides=1,\n",
        "            activation=params_nn_['conv_activation'],\n",
        "            use_bias=False))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    nn.add(layers.ReLU(6.))\n",
        "    nn.add(Conv2D(params_nn_['conv_filter'], kernel_size=params_nn_['conv_kernel'], padding='same', activation=params_nn_['conv_activation'], use_bias=False, \n",
        "                strides=1))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    return nn\n",
        "    \n",
        "def nn_cl_fun_final():\n",
        "    nn = Sequential()\n",
        "    nn.add(Conv2D(params_nn_['conv_filter'], kernel_size=params_nn_['conv_kernel'], input_shape=(32, 32, 3), strides=1, activation=params_nn_['conv_activation'], padding='same'))\n",
        "    nn.add(BatchNormalization(epsilon=1e-3, momentum=0.999))\n",
        "    nn.add(layers.ReLU(6.))\n",
        "    nn = SepConv_final(nn)\n",
        "    for i in range(params_nn_['cell_magnitude1']):\n",
        "        nn = MBConv_final(nn)\n",
        "    if params_nn_['dropout'] > 0.5:\n",
        "        nn.add(Dropout(params_nn_['dropout_rate'], seed=123))\n",
        "    for i in range(params_nn_['cell_magnitude2']):\n",
        "        nn = MBConv_final(nn)\n",
        "    nn.add(MaxPool2D(pool_size=(2, 2), padding='same', strides=1))\n",
        "    nn.add(Flatten())\n",
        "    nn.add(Dense(10, activation='softmax'))\n",
        "    nn.compile(loss=\"categorical_crossentropy\", optimizer=params_nn_['optimizer'], metrics=['accuracy'])\n",
        "    return nn\n",
        "        \n",
        "# es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
        "# nn = KerasClassifier(build_fn=nn_cl_fun, epochs=params_nn_['epochs'], batch_size=params_nn_['batch_size'],\n",
        "#                          verbose=0)\n",
        "model_final = nn_cl_fun()\n",
        "loss, accuracy = model_final.evaluate(x_test, y_test, verbose=1)\n",
        "print('Accuracy: %f' % (accuracy))\n",
        "print('Loss: %f' % (loss))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model6 = Sequential()\n",
        "model6.add(Conv2D(filters=84, kernel_size=3, input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
        "model6.add(MaxPool2D(pool_size=2))\n",
        "model6.add(Conv2D(filters=84, kernel_size=3, input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
        "model6.add(MaxPool2D(pool_size=2))\n",
        "model6.add(Conv2D(filters=84, kernel_size=3, input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
        "model6.add(MaxPool2D(pool_size=2))\n",
        "model6.add(Conv2D(filters=84, kernel_size=3, input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
        "model6.add(MaxPool2D(pool_size=2))\n",
        "model6.add(Conv2D(filters=84, kernel_size=3, input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
        "#model6.add(MaxPool2D(pool_size=2))\n",
        "model6.add(Conv2D(filters=84, kernel_size=3, input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
        "model6.add(MaxPool2D(pool_size=2))\n",
        "model6.add(Flatten())\n",
        "model6.add(Dense(10, activation='softmax'))\n",
        "model6.summary()\n",
        "\n",
        "model6.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model6.fit(x_train, y_train, epochs=10, batch_size=64, verbose=1, validation_data=(x_test, y_test),shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OTNUVhsVyH-",
        "outputId": "f05b3ce9-8978-4141-efde-f30c463faa9b"
      },
      "id": "9OTNUVhsVyH-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_85\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_566 (Conv2D)         (None, 32, 32, 84)        2352      \n",
            "                                                                 \n",
            " max_pooling2d_87 (MaxPoolin  (None, 16, 16, 84)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_567 (Conv2D)         (None, 16, 16, 84)        63588     \n",
            "                                                                 \n",
            " max_pooling2d_88 (MaxPoolin  (None, 8, 8, 84)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_568 (Conv2D)         (None, 8, 8, 84)          63588     \n",
            "                                                                 \n",
            " max_pooling2d_89 (MaxPoolin  (None, 4, 4, 84)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_569 (Conv2D)         (None, 4, 4, 84)          63588     \n",
            "                                                                 \n",
            " max_pooling2d_90 (MaxPoolin  (None, 2, 2, 84)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_570 (Conv2D)         (None, 2, 2, 84)          63588     \n",
            "                                                                 \n",
            " conv2d_571 (Conv2D)         (None, 2, 2, 84)          63588     \n",
            "                                                                 \n",
            " max_pooling2d_91 (MaxPoolin  (None, 1, 1, 84)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_85 (Flatten)        (None, 84)                0         \n",
            "                                                                 \n",
            " dense_85 (Dense)            (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 321,142\n",
            "Trainable params: 321,142\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Accuracy: 0.099900\n",
            "Loss: 2.302158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline model with dropout and data augmentation on the cifar10 dataset\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10\n",
        "# from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "# from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras import regularizers\n",
        "from keras import Model\n",
        "from keras.activations import relu\n",
        "from keras import layers as Layers \n",
        "\n",
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\t(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "\t# one hot encode target values\n",
        "\ttrainY = np_utils.to_categorical(trainY)\n",
        "\ttestY = np_utils.to_categorical(testY)\n",
        "\treturn trainX, trainY, testX, testY\n",
        " \n",
        "# scale pixels\n",
        "def prep_pixels(train, test):\n",
        "\t# convert from integers to floats\n",
        "\ttrain_norm = train.astype('float32')\n",
        "\ttest_norm = test.astype('float32')\n",
        "\t# normalize to range 0-1\n",
        "\ttrain_norm = train_norm / 255.0\n",
        "\ttest_norm = test_norm / 255.0\n",
        "\t# return normalized images\n",
        "\treturn train_norm, test_norm\n",
        "\n",
        "def identity_block(x, filter):\n",
        "    # copy tensor to variable called x_skip\n",
        "    x_skip = x\n",
        "    # Layer 1\n",
        "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    # Layer 2\n",
        "    x = tf.keras.layers.Conv2D(filter, (3,3), padding = 'same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
        "    # Add Residue\n",
        "    x = tf.keras.layers.Add()([x, x_skip])     \n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    return model\n",
        "\n",
        " \n",
        "# define cnn model\n",
        "def define_model():\n",
        "\n",
        "\tweight_decay = 0.001\n",
        "\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3), kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "\tmodel.add(BatchNormalization())\n",
        "  # add residual\n",
        "\tidentity_block(model, 64)\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.3))\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\t\n",
        "  # add residual\n",
        "\tidentity_block(model, 128)\n",
        "\tmodel.add(GlobalAveragePooling2D())\n",
        "\tmodel.add(Dropout(0.4))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(Dense(10, activation='softmax'))\n",
        "\t# compile model\n",
        "\topt = tf.keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9)\n",
        "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\treturn model\n",
        " \n",
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(211)\n",
        "\tpyplot.title('Cross Entropy Loss')\n",
        "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "\t# plot accuracy\n",
        "\tpyplot.subplot(212)\n",
        "\tpyplot.title('Classification Accuracy')\n",
        "\tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "\tpyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "\t# save plot to file\n",
        "\tfilename = sys.argv[0].split('/')[-1]\n",
        "\tpyplot.savefig(filename + '_plot.png')\n",
        "\tpyplot.close()\n",
        " \n",
        "# run the test harness for evaluating a model\n",
        "def run_test_harness():\n",
        "  # load dataset\n",
        "  trainX, trainY, testX, testY = load_dataset()\n",
        "  # prepare pixel data\n",
        "  trainX, testX = prep_pixels(trainX, testX)\n",
        "  # define model\n",
        "  model = define_model()\n",
        "  # evaluate model\n",
        "  history = model.fit(trainX, trainY, epochs=100, batch_size=64, verbose=1, validation_data=(testX, testY),shuffle=True)\n",
        "  # loss, acc = model.evaluate(testX, testY, verbose=0)\n",
        "  # print('> %.3f' % (acc * 100.0))\n",
        "  # learning curves\n",
        "  summarize_diagnostics(history)\n",
        " \n",
        "# entry point, run the test harness\n",
        "run_test_harness()"
      ],
      "metadata": {
        "id": "Jq9zATY_cUvq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "c4a82a98-392d-4cb5-903b-f48b4ecb47ad"
      },
      "id": "Jq9zATY_cUvq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e38ed530626e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;31m# entry point, run the test harness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m \u001b[0mrun_test_harness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-e38ed530626e>\u001b[0m in \u001b[0;36mrun_test_harness\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_pixels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m   \u001b[0;31m# define model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m   \u001b[0;31m# evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m   \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-e38ed530626e>\u001b[0m in \u001b[0;36mdefine_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m   \u001b[0;31m# add residual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0midentity_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-e38ed530626e>\u001b[0m in \u001b[0;36midentity_block\u001b[0;34m(x, filter)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mx_skip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Layer 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# have a `shape` attribute.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Inputs to a layer should be tensors. Got: {x}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Inputs to a layer should be tensors. Got: <keras.engine.sequential.Sequential object at 0x7fc279634810>"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "data augmentation"
      ],
      "metadata": {
        "id": "DaXt-NzxHrKN"
      },
      "id": "DaXt-NzxHrKN"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from scipy.ndimage import rotate\n",
        "sns.set(color_codes=True)\n",
        "\n",
        "\n",
        "def translate(img, shift, dir, r):\n",
        "    directions = ['right', 'left', 'down', 'up']\n",
        "    rolls = [True, False]\n",
        "    direction = directions[dir]\n",
        "    roll = rolls[r]\n",
        "    img = img.copy()\n",
        "    if direction == 'right':\n",
        "        right_slice = img[:, -shift:].copy()\n",
        "        img[:, shift:] = img[:, :-shift]\n",
        "        if roll:\n",
        "            img[:,:shift] = np.fliplr(right_slice)\n",
        "    if direction == 'left':\n",
        "        left_slice = img[:, :shift].copy()\n",
        "        img[:, :-shift] = img[:, shift:]\n",
        "        if roll:\n",
        "            img[:, -shift:] = left_slice\n",
        "    if direction == 'down':\n",
        "        down_slice = img[-shift:, :].copy()\n",
        "        img[shift:, :] = img[:-shift,:]\n",
        "        if roll:\n",
        "            img[:shift, :] = down_slice\n",
        "    if direction == 'up':\n",
        "        upper_slice = img[:shift, :].copy()\n",
        "        img[:-shift, :] = img[shift:, :]\n",
        "        if roll:\n",
        "            img[-shift:,:] = upper_slice\n",
        "    return img\n",
        "\n",
        "def random_crop(img, crop_size):\n",
        "    assert crop_size[0] <= img.shape[0] and crop_size[1] <= img.shape[1], \"Crop size should be less than image size\"\n",
        "    img = img.copy()\n",
        "    w, h = img.shape[:2]\n",
        "    x, y = np.random.randint(h-crop_size[0]), np.random.randint(w-crop_size[1])\n",
        "    img = img[y:y+crop_size[0], x:x+crop_size[1]]\n",
        "    return img\n",
        "\n",
        "def rotate_img(img, angle, bg_patch):\n",
        "    assert len(img.shape) <= 3, \"Incorrect image shape\"\n",
        "    rgb = len(img.shape) == 3\n",
        "    if rgb:\n",
        "        bg_color = np.mean(img[:bg_patch[0], :bg_patch[1], :], axis=(0,1))\n",
        "    else:\n",
        "        bg_color = np.mean(img[:bg_patch[0], :bg_patch[1]])\n",
        "    img = rotate(img, angle, reshape=False)\n",
        "    mask = [img <= 0, np.any(img <= 0, axis=-1)][rgb]\n",
        "    img[mask] = bg_color\n",
        "    return img\n",
        "\n",
        "def gaussian_noise(img, mean=0, sigma=0.03):\n",
        "    img = img.copy()\n",
        "    noise = np.random.normal(mean, sigma, img.shape)\n",
        "    mask_overflow_upper = img+noise >= 1.0\n",
        "    mask_overflow_lower = img+noise < 0\n",
        "    noise[mask_overflow_upper] = 1.0\n",
        "    noise[mask_overflow_lower] = 0\n",
        "    img += noise\n",
        "    return img\n",
        "\n",
        "\n",
        "def distort(img, orientation='horizontal', func=np.sin, x_scale=0.05, y_scale=5):\n",
        "    assert orientation[:3] in ['hor', 'ver'], \"dist_orient should be 'horizontal'|'vertical'\"\n",
        "    assert func in [np.sin, np.cos], \"supported functions are np.sin and np.cos\"\n",
        "    assert 0.00 <= x_scale <= 0.1, \"x_scale should be in [0.0, 0.1]\"\n",
        "    assert 0 <= y_scale <= min(img.shape[0], img.shape[1]), \"y_scale should be less then image size\"\n",
        "    img_dist = img.copy()\n",
        "    \n",
        "    def shift(x):\n",
        "        return int(y_scale * func(np.pi * x * x_scale))\n",
        "    \n",
        "    for c in range(3):\n",
        "        for i in range(img.shape[orientation.startswith('ver')]):\n",
        "            if orientation.startswith('ver'):\n",
        "                img_dist[:, i, c] = np.roll(img[:, i, c], shift(i))\n",
        "            else:\n",
        "                img_dist[i, :, c] = np.roll(img[i, :, c], shift(i))\n",
        "            \n",
        "    return img_dist\n",
        "\n",
        "\n",
        "def change_channel_ratio(img, c, ratio):\n",
        "    channels = ['r', 'g', 'b']\n",
        "    channel = channels[c]\n",
        "    img = img.copy()\n",
        "    ci = 'rgb'.index(channel)\n",
        "    img[:, :, ci] *= ratio\n",
        "    return img\n",
        "\n",
        "\n",
        "def change_channel_ratio_gauss(img, c, mean, sigma):\n",
        "    channels = ['r', 'g', 'b']\n",
        "    channel = channels[c]\n",
        "    img = img.copy()\n",
        "    ci = 'rgb'.index(channel)\n",
        "    img[:, :, ci] = gaussian_noise(img[:, :, ci], mean=mean, sigma=sigma)\n",
        "    return img\n",
        "\n",
        "def cutoff(img, width, height):\n",
        "  img_new = img.copy()\n",
        "  w, h = img.shape[:2]\n",
        "  x, y = np.random.randint(h), np.random.randint(w)\n",
        "  img_new[y:y+width, x:x+height] = 0\n",
        "  return img_new\n"
      ],
      "metadata": {
        "id": "NFFnCmoWHqlG"
      },
      "id": "NFFnCmoWHqlG",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YRqyHqfvOuhf"
      },
      "id": "YRqyHqfvOuhf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data processing"
      ],
      "metadata": {
        "id": "W4xFZjFHRbAw"
      },
      "id": "W4xFZjFHRbAw"
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline model with dropout and data augmentation on the cifar10 dataset\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10\n",
        "# from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "# from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras import regularizers\n",
        "from keras import Model\n",
        "from keras.activations import relu\n",
        "from keras import layers as Layers \n",
        "from keras.regularizers import l2\n",
        "import h5py\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import keras as k\n",
        "import cv2\n",
        "\n",
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\t(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "\t# one hot encode target values\n",
        "\ttrainY = np_utils.to_categorical(trainY)\n",
        "\ttestY = np_utils.to_categorical(testY)\n",
        "\treturn trainX, trainY, testX, testY\n",
        " \n",
        "# scale pixels\n",
        "def prep_pixels(train, test):\n",
        "  # convert from integers to floats\n",
        "  train_norm = train.astype('float32')\n",
        "  test_norm = test.astype('float32')\n",
        "  # # normalize to range 0-1\n",
        "  # train_norm = train_norm / 255.0\n",
        "  # test_norm = test_norm / 255.0\n",
        "  #z-score\n",
        "  mean = np.mean(trainX,axis=(0,1,2,3))\n",
        "  std = np.std(trainX,axis=(0,1,2,3))\n",
        "  x_train = (trainX-mean)/(std+1e-7)\n",
        "  x_test = (testX-mean)/(std+1e-7)\n",
        "  # return normalized images\n",
        "  return train_norm, test_norm\n",
        "\n",
        "# load dataset\n",
        "trainX, trainY, testX, testY = load_dataset()\n",
        "\n",
        "# prepare pixel data\n",
        "trainX, testX = prep_pixels(trainX, testX)\n",
        "\n",
        "# set up image augmentation\n",
        "# datagen = ImageDataGenerator(\n",
        "#     featurewise_center=True, samplewise_center=True,\n",
        "#     featurewise_std_normalization=True, samplewise_std_normalization=True,\n",
        "#     rotation_range=90, width_shift_range=0.2,\n",
        "#     height_shift_range=0.2, brightness_range=None, shear_range=0.2, zoom_range=0.2,\n",
        "#     channel_shift_range=0.2, fill_mode='nearest', cval=0.2,\n",
        "#     horizontal_flip=True, vertical_flip=True, rescale=1.2\n",
        "# )\n",
        "# datagen.fit(trainX)\n",
        "x_train = trainX\n",
        "y_train = trainY\n",
        "\n",
        "count = 0\n",
        "for img in trainX[0:1000]:\n",
        "  imgs_distorted = []\n",
        "  for ori in ['ver', 'hor']:\n",
        "    for x_param in [0.01, 0.02, 0.03, 0.04]:\n",
        "      for y_param in [2, 4, 6, 8, 10]:\n",
        "          imgs_distorted.append(distort(img, orientation=ori, x_scale=x_param, y_scale=y_param))\n",
        "  imgs_translate = translate(img, shift=np.random.randint(1,10), dir=np.random.randint(0,3), r=np.random.randint(0,1))\n",
        "  imgs_crop = random_crop(img, crop_size=(np.random.randint(1,10), np.random.randint(1,10)))\n",
        "  imgs_crop_resized = cv2.resize(imgs_crop, dsize=(32, 32), interpolation=cv2. INTER_CUBIC)\n",
        "  imgs_rotate = rotate_img(img, angle=np.random.randint(10,30), bg_patch=(np.random.randint(1,5),np.random.randint(1,5)))\n",
        "  imgs_gaussian = gaussian_noise(img, mean=0, sigma=np.random.random_sample())\n",
        "  imgs_change_channel_ratio = change_channel_ratio(img, c=np.random.randint(0,2), ratio=np.random.random_sample())\n",
        "  imgs_change_channel_ratio_gauss = change_channel_ratio_gauss(img, c=np.random.randint(0,2), mean=0, sigma=np.random.random_sample())\n",
        "  img_cutoff = cutoff(img, width=np.random.randint(3,12), height=np.random.randint(3,12))\n",
        "  x_train = np.append(x_train, imgs_distorted, axis=0)\n",
        "  for i in range(len(imgs_distorted)):\n",
        "    y_train = np.append(y_train, [trainY[count]], axis=0)\n",
        "  x_train = np.append(x_train, [imgs_translate], axis=0)\n",
        "  for i in range(len([imgs_translate])):\n",
        "    y_train = np.append(y_train, [trainY[count]], axis=0)\n",
        "  x_train = np.append(x_train, [imgs_crop_resized], axis=0)\n",
        "  for i in range(len([imgs_crop_resized])):\n",
        "    y_train = np.append(y_train, [trainY[count]], axis=0)\n",
        "  x_train = np.append(x_train, [imgs_rotate], axis=0)\n",
        "  for i in range(len([imgs_rotate])):\n",
        "    y_train = np.append(y_train, [trainY[count]], axis=0)\n",
        "  x_train = np.append(x_train, [imgs_gaussian], axis=0)\n",
        "  for i in range(len([imgs_gaussian])):\n",
        "    y_train = np.append(y_train, [trainY[count]], axis=0)\n",
        "  x_train = np.append(x_train, [imgs_change_channel_ratio], axis=0)\n",
        "  for i in range(len([imgs_change_channel_ratio])):\n",
        "    y_train = np.append(y_train, [trainY[count]], axis=0)\n",
        "  x_train = np.append(x_train, [imgs_change_channel_ratio_gauss], axis=0)\n",
        "  for i in range(len([imgs_change_channel_ratio_gauss])):\n",
        "    y_train = np.append(y_train, [trainY[count]], axis=0)\n",
        "  x_train = np.append(x_train, [img_cutoff], axis=0)\n",
        "  for i in range(len([img_cutoff])):\n",
        "    y_train = np.append(y_train, [trainY[count]], axis=0)\n",
        "  count += 1\n",
        "  print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HREvayRRdV3",
        "outputId": "ab0cd0ba-7cf4-491c-dd74-998ea6143f35"
      },
      "id": "2HREvayRRdV3",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n",
            "533\n",
            "534\n",
            "535\n",
            "536\n",
            "537\n",
            "538\n",
            "539\n",
            "540\n",
            "541\n",
            "542\n",
            "543\n",
            "544\n",
            "545\n",
            "546\n",
            "547\n",
            "548\n",
            "549\n",
            "550\n",
            "551\n",
            "552\n",
            "553\n",
            "554\n",
            "555\n",
            "556\n",
            "557\n",
            "558\n",
            "559\n",
            "560\n",
            "561\n",
            "562\n",
            "563\n",
            "564\n",
            "565\n",
            "566\n",
            "567\n",
            "568\n",
            "569\n",
            "570\n",
            "571\n",
            "572\n",
            "573\n",
            "574\n",
            "575\n",
            "576\n",
            "577\n",
            "578\n",
            "579\n",
            "580\n",
            "581\n",
            "582\n",
            "583\n",
            "584\n",
            "585\n",
            "586\n",
            "587\n",
            "588\n",
            "589\n",
            "590\n",
            "591\n",
            "592\n",
            "593\n",
            "594\n",
            "595\n",
            "596\n",
            "597\n",
            "598\n",
            "599\n",
            "600\n",
            "601\n",
            "602\n",
            "603\n",
            "604\n",
            "605\n",
            "606\n",
            "607\n",
            "608\n",
            "609\n",
            "610\n",
            "611\n",
            "612\n",
            "613\n",
            "614\n",
            "615\n",
            "616\n",
            "617\n",
            "618\n",
            "619\n",
            "620\n",
            "621\n",
            "622\n",
            "623\n",
            "624\n",
            "625\n",
            "626\n",
            "627\n",
            "628\n",
            "629\n",
            "630\n",
            "631\n",
            "632\n",
            "633\n",
            "634\n",
            "635\n",
            "636\n",
            "637\n",
            "638\n",
            "639\n",
            "640\n",
            "641\n",
            "642\n",
            "643\n",
            "644\n",
            "645\n",
            "646\n",
            "647\n",
            "648\n",
            "649\n",
            "650\n",
            "651\n",
            "652\n",
            "653\n",
            "654\n",
            "655\n",
            "656\n",
            "657\n",
            "658\n",
            "659\n",
            "660\n",
            "661\n",
            "662\n",
            "663\n",
            "664\n",
            "665\n",
            "666\n",
            "667\n",
            "668\n",
            "669\n",
            "670\n",
            "671\n",
            "672\n",
            "673\n",
            "674\n",
            "675\n",
            "676\n",
            "677\n",
            "678\n",
            "679\n",
            "680\n",
            "681\n",
            "682\n",
            "683\n",
            "684\n",
            "685\n",
            "686\n",
            "687\n",
            "688\n",
            "689\n",
            "690\n",
            "691\n",
            "692\n",
            "693\n",
            "694\n",
            "695\n",
            "696\n",
            "697\n",
            "698\n",
            "699\n",
            "700\n",
            "701\n",
            "702\n",
            "703\n",
            "704\n",
            "705\n",
            "706\n",
            "707\n",
            "708\n",
            "709\n",
            "710\n",
            "711\n",
            "712\n",
            "713\n",
            "714\n",
            "715\n",
            "716\n",
            "717\n",
            "718\n",
            "719\n",
            "720\n",
            "721\n",
            "722\n",
            "723\n",
            "724\n",
            "725\n",
            "726\n",
            "727\n",
            "728\n",
            "729\n",
            "730\n",
            "731\n",
            "732\n",
            "733\n",
            "734\n",
            "735\n",
            "736\n",
            "737\n",
            "738\n",
            "739\n",
            "740\n",
            "741\n",
            "742\n",
            "743\n",
            "744\n",
            "745\n",
            "746\n",
            "747\n",
            "748\n",
            "749\n",
            "750\n",
            "751\n",
            "752\n",
            "753\n",
            "754\n",
            "755\n",
            "756\n",
            "757\n",
            "758\n",
            "759\n",
            "760\n",
            "761\n",
            "762\n",
            "763\n",
            "764\n",
            "765\n",
            "766\n",
            "767\n",
            "768\n",
            "769\n",
            "770\n",
            "771\n",
            "772\n",
            "773\n",
            "774\n",
            "775\n",
            "776\n",
            "777\n",
            "778\n",
            "779\n",
            "780\n",
            "781\n",
            "782\n",
            "783\n",
            "784\n",
            "785\n",
            "786\n",
            "787\n",
            "788\n",
            "789\n",
            "790\n",
            "791\n",
            "792\n",
            "793\n",
            "794\n",
            "795\n",
            "796\n",
            "797\n",
            "798\n",
            "799\n",
            "800\n",
            "801\n",
            "802\n",
            "803\n",
            "804\n",
            "805\n",
            "806\n",
            "807\n",
            "808\n",
            "809\n",
            "810\n",
            "811\n",
            "812\n",
            "813\n",
            "814\n",
            "815\n",
            "816\n",
            "817\n",
            "818\n",
            "819\n",
            "820\n",
            "821\n",
            "822\n",
            "823\n",
            "824\n",
            "825\n",
            "826\n",
            "827\n",
            "828\n",
            "829\n",
            "830\n",
            "831\n",
            "832\n",
            "833\n",
            "834\n",
            "835\n",
            "836\n",
            "837\n",
            "838\n",
            "839\n",
            "840\n",
            "841\n",
            "842\n",
            "843\n",
            "844\n",
            "845\n",
            "846\n",
            "847\n",
            "848\n",
            "849\n",
            "850\n",
            "851\n",
            "852\n",
            "853\n",
            "854\n",
            "855\n",
            "856\n",
            "857\n",
            "858\n",
            "859\n",
            "860\n",
            "861\n",
            "862\n",
            "863\n",
            "864\n",
            "865\n",
            "866\n",
            "867\n",
            "868\n",
            "869\n",
            "870\n",
            "871\n",
            "872\n",
            "873\n",
            "874\n",
            "875\n",
            "876\n",
            "877\n",
            "878\n",
            "879\n",
            "880\n",
            "881\n",
            "882\n",
            "883\n",
            "884\n",
            "885\n",
            "886\n",
            "887\n",
            "888\n",
            "889\n",
            "890\n",
            "891\n",
            "892\n",
            "893\n",
            "894\n",
            "895\n",
            "896\n",
            "897\n",
            "898\n",
            "899\n",
            "900\n",
            "901\n",
            "902\n",
            "903\n",
            "904\n",
            "905\n",
            "906\n",
            "907\n",
            "908\n",
            "909\n",
            "910\n",
            "911\n",
            "912\n",
            "913\n",
            "914\n",
            "915\n",
            "916\n",
            "917\n",
            "918\n",
            "919\n",
            "920\n",
            "921\n",
            "922\n",
            "923\n",
            "924\n",
            "925\n",
            "926\n",
            "927\n",
            "928\n",
            "929\n",
            "930\n",
            "931\n",
            "932\n",
            "933\n",
            "934\n",
            "935\n",
            "936\n",
            "937\n",
            "938\n",
            "939\n",
            "940\n",
            "941\n",
            "942\n",
            "943\n",
            "944\n",
            "945\n",
            "946\n",
            "947\n",
            "948\n",
            "949\n",
            "950\n",
            "951\n",
            "952\n",
            "953\n",
            "954\n",
            "955\n",
            "956\n",
            "957\n",
            "958\n",
            "959\n",
            "960\n",
            "961\n",
            "962\n",
            "963\n",
            "964\n",
            "965\n",
            "966\n",
            "967\n",
            "968\n",
            "969\n",
            "970\n",
            "971\n",
            "972\n",
            "973\n",
            "974\n",
            "975\n",
            "976\n",
            "977\n",
            "978\n",
            "979\n",
            "980\n",
            "981\n",
            "982\n",
            "983\n",
            "984\n",
            "985\n",
            "986\n",
            "987\n",
            "988\n",
            "989\n",
            "990\n",
            "991\n",
            "992\n",
            "993\n",
            "994\n",
            "995\n",
            "996\n",
            "997\n",
            "998\n",
            "999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "using only cutoff"
      ],
      "metadata": {
        "id": "ImWxo3GMundb"
      },
      "id": "ImWxo3GMundb"
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_cutoff = trainX\n",
        "y_train_cutoff = trainY\n",
        "\n",
        "count = 0\n",
        "for img in trainX[0:50000]:\n",
        "  img_cutoff = cutoff(img, width=np.random.randint(1,12), height=np.random.randint(1,12))\n",
        "  x_train_cutoff = np.append(x_train_cutoff, [img_cutoff], axis=0)\n",
        "  for i in range(len([img_cutoff])):\n",
        "    y_train_cutoff = np.append(y_train_cutoff, [trainY[count]], axis=0)\n",
        "  count += 1\n",
        "  print(count)"
      ],
      "metadata": {
        "id": "P1TAbLJzuiK7"
      },
      "id": "P1TAbLJzuiK7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##################################################### build model #####################################################################################\n",
        "inputs = tf.keras.Input(shape=(32, 32, 3))\n",
        "weight_decay = 1e-5\n",
        "\n",
        "x = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(inputs)\n",
        "x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x) \n",
        "x = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x) \n",
        "\n",
        "x = MaxPooling2D(2, 2)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "x = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x) \n",
        "\n",
        "# add residual\n",
        "x_skip = x\n",
        "# Layer 1\n",
        "x = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x) \n",
        "# x = tf.keras.layers.Activation('relu')(x)\n",
        "# Layer 2\n",
        "x = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x) \n",
        "# Add Residue\n",
        "x = tf.keras.layers.Add()([x, x_skip])     \n",
        "# x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "x = MaxPooling2D(2, 2)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "x = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x) \n",
        "\n",
        "# add residual\n",
        "x_skip = x\n",
        "# Layer 1\n",
        "x = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x) \n",
        "# x = tf.keras.layers.Activation('relu')(x)\n",
        "# Layer 2\n",
        "x = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x) \n",
        "# Add Residue\n",
        "x = tf.keras.layers.Add()([x, x_skip])     \n",
        "x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "x = MaxPooling2D(2, 2)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "x = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x) \n",
        "\n",
        "# add residual\n",
        "x_skip = x\n",
        "# Layer 1\n",
        "x = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x) \n",
        "# x = tf.keras.layers.Activation('relu')(x)\n",
        "# Layer 2\n",
        "x = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x) \n",
        "# Add Residue\n",
        "x = tf.keras.layers.Add()([x, x_skip])     \n",
        "x = tf.keras.layers.Activation('relu')(x) \n",
        "\n",
        "x = MaxPooling2D(2, 2)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "x = Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x) \n",
        "\n",
        "# add residual\n",
        "x_skip = x\n",
        "# Layer 1\n",
        "x = Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x) \n",
        "# x = tf.keras.layers.Activation('relu')(x)\n",
        "# Layer 2\n",
        "x = Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(weight_decay))(x)\n",
        "x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x)\n",
        "# Add Residue\n",
        "x = tf.keras.layers.Add()([x, x_skip])     \n",
        "x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu', kernel_initializer='he_uniform')(x)\n",
        "x = BatchNormalization(epsilon=1e-3, momentum=0.999)(x) \n",
        "x = Dropout(0.2)(x)\n",
        "outputs = Dense(10, activation='softmax')(x)\n",
        "##################################################### build model #####################################################################################\n",
        "\n",
        "# compile model\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "#     initial_learning_rate=0.1,\n",
        "#     decay_steps=10000,\n",
        "#     decay_rate=1.1)\n",
        "learning_rate = 0.1\n",
        "epochs = 100\n",
        "decay_rate = learning_rate / epochs\n",
        "opt = tf.keras.optimizers.SGD(learning_rate=learning_rate , decay=decay_rate, momentum=0.9)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(len(x_train_cutoff))\n",
        "print(len(y_train_cutoff))\n",
        "\n",
        "# evaluate model\n",
        "# history = model.fit(x_train, y_train, epochs=epochs, batch_size=128, verbose=1, validation_data=(testX, testY), shuffle=True)\n",
        "history = model.fit(x_train_cutoff, y_train_cutoff, epochs=epochs, batch_size=64, verbose=1, validation_data=(testX, testY), shuffle=True)\n",
        "summarize_diagnostics(history)"
      ],
      "metadata": {
        "id": "xsQ8aWzBMQEq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "adae9c9d-8a91-4f87-edfd-adde799224a2"
      },
      "id": "xsQ8aWzBMQEq",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59999\n",
            "59999\n",
            "Epoch 1/100\n",
            "469/469 [==============================] - 15s 26ms/step - loss: 1.8254 - accuracy: 0.3790 - val_loss: 67.3709 - val_accuracy: 0.1200\n",
            "Epoch 2/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 1.2892 - accuracy: 0.5692 - val_loss: 4.0034 - val_accuracy: 0.2280\n",
            "Epoch 3/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 1.0711 - accuracy: 0.6513 - val_loss: 1.8057 - val_accuracy: 0.5004\n",
            "Epoch 4/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.9406 - accuracy: 0.6994 - val_loss: 1.4862 - val_accuracy: 0.5748\n",
            "Epoch 5/100\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.8439 - accuracy: 0.7342 - val_loss: 0.8935 - val_accuracy: 0.7227\n",
            "Epoch 6/100\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.7677 - accuracy: 0.7642 - val_loss: 0.8825 - val_accuracy: 0.7229\n",
            "Epoch 7/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.7057 - accuracy: 0.7851 - val_loss: 0.8122 - val_accuracy: 0.7507\n",
            "Epoch 8/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.6528 - accuracy: 0.8031 - val_loss: 0.8705 - val_accuracy: 0.7350\n",
            "Epoch 9/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.6031 - accuracy: 0.8215 - val_loss: 0.8388 - val_accuracy: 0.7431\n",
            "Epoch 10/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.5608 - accuracy: 0.8374 - val_loss: 0.7649 - val_accuracy: 0.7697\n",
            "Epoch 11/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.5265 - accuracy: 0.8496 - val_loss: 0.7892 - val_accuracy: 0.7656\n",
            "Epoch 12/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.4909 - accuracy: 0.8608 - val_loss: 0.7454 - val_accuracy: 0.7856\n",
            "Epoch 13/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.4622 - accuracy: 0.8707 - val_loss: 0.7063 - val_accuracy: 0.7960\n",
            "Epoch 14/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.4380 - accuracy: 0.8791 - val_loss: 0.7394 - val_accuracy: 0.7919\n",
            "Epoch 15/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.4131 - accuracy: 0.8861 - val_loss: 0.7281 - val_accuracy: 0.7972\n",
            "Epoch 16/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.3905 - accuracy: 0.8972 - val_loss: 0.7014 - val_accuracy: 0.8060\n",
            "Epoch 17/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.3690 - accuracy: 0.9035 - val_loss: 0.6801 - val_accuracy: 0.8188\n",
            "Epoch 18/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.3499 - accuracy: 0.9097 - val_loss: 0.7237 - val_accuracy: 0.8069\n",
            "Epoch 19/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.3271 - accuracy: 0.9177 - val_loss: 0.7885 - val_accuracy: 0.8028\n",
            "Epoch 20/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.3154 - accuracy: 0.9220 - val_loss: 0.7590 - val_accuracy: 0.8071\n",
            "Epoch 21/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.2984 - accuracy: 0.9283 - val_loss: 0.7312 - val_accuracy: 0.8122\n",
            "Epoch 22/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.2859 - accuracy: 0.9326 - val_loss: 0.7662 - val_accuracy: 0.8113\n",
            "Epoch 23/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.2735 - accuracy: 0.9365 - val_loss: 0.8217 - val_accuracy: 0.8030\n",
            "Epoch 24/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.2571 - accuracy: 0.9416 - val_loss: 0.7877 - val_accuracy: 0.8103\n",
            "Epoch 25/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.2515 - accuracy: 0.9452 - val_loss: 0.7746 - val_accuracy: 0.8174\n",
            "Epoch 26/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.2449 - accuracy: 0.9466 - val_loss: 0.7787 - val_accuracy: 0.8215\n",
            "Epoch 27/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.2299 - accuracy: 0.9519 - val_loss: 0.8215 - val_accuracy: 0.8168\n",
            "Epoch 28/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.2248 - accuracy: 0.9534 - val_loss: 0.7996 - val_accuracy: 0.8185\n",
            "Epoch 29/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.2163 - accuracy: 0.9563 - val_loss: 0.8218 - val_accuracy: 0.8166\n",
            "Epoch 30/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.2105 - accuracy: 0.9589 - val_loss: 0.8776 - val_accuracy: 0.8113\n",
            "Epoch 31/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.2061 - accuracy: 0.9599 - val_loss: 0.8339 - val_accuracy: 0.8202\n",
            "Epoch 32/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1950 - accuracy: 0.9637 - val_loss: 0.8939 - val_accuracy: 0.8129\n",
            "Epoch 33/100\n",
            "469/469 [==============================] - 11s 25ms/step - loss: 0.1951 - accuracy: 0.9641 - val_loss: 0.8703 - val_accuracy: 0.8149\n",
            "Epoch 34/100\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.1888 - accuracy: 0.9666 - val_loss: 0.8400 - val_accuracy: 0.8218\n",
            "Epoch 35/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1859 - accuracy: 0.9670 - val_loss: 0.8528 - val_accuracy: 0.8223\n",
            "Epoch 36/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1790 - accuracy: 0.9693 - val_loss: 0.8655 - val_accuracy: 0.8236\n",
            "Epoch 37/100\n",
            "469/469 [==============================] - 11s 25ms/step - loss: 0.1730 - accuracy: 0.9724 - val_loss: 0.8816 - val_accuracy: 0.8216\n",
            "Epoch 38/100\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.1713 - accuracy: 0.9730 - val_loss: 0.8975 - val_accuracy: 0.8224\n",
            "Epoch 39/100\n",
            "469/469 [==============================] - 11s 25ms/step - loss: 0.1686 - accuracy: 0.9738 - val_loss: 0.9307 - val_accuracy: 0.8193\n",
            "Epoch 40/100\n",
            "469/469 [==============================] - 11s 25ms/step - loss: 0.1670 - accuracy: 0.9745 - val_loss: 0.9501 - val_accuracy: 0.8158\n",
            "Epoch 41/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1602 - accuracy: 0.9766 - val_loss: 0.9065 - val_accuracy: 0.8228\n",
            "Epoch 42/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1604 - accuracy: 0.9759 - val_loss: 0.9133 - val_accuracy: 0.8260\n",
            "Epoch 43/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1587 - accuracy: 0.9777 - val_loss: 0.9096 - val_accuracy: 0.8224\n",
            "Epoch 44/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1582 - accuracy: 0.9774 - val_loss: 0.9279 - val_accuracy: 0.8199\n",
            "Epoch 45/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1495 - accuracy: 0.9809 - val_loss: 0.9141 - val_accuracy: 0.8247\n",
            "Epoch 46/100\n",
            "469/469 [==============================] - 11s 25ms/step - loss: 0.1520 - accuracy: 0.9800 - val_loss: 0.9434 - val_accuracy: 0.8212\n",
            "Epoch 47/100\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.1502 - accuracy: 0.9795 - val_loss: 0.9656 - val_accuracy: 0.8172\n",
            "Epoch 48/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1476 - accuracy: 0.9811 - val_loss: 0.9485 - val_accuracy: 0.8211\n",
            "Epoch 49/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1432 - accuracy: 0.9822 - val_loss: 0.9243 - val_accuracy: 0.8243\n",
            "Epoch 50/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1417 - accuracy: 0.9828 - val_loss: 0.9415 - val_accuracy: 0.8259\n",
            "Epoch 51/100\n",
            "469/469 [==============================] - 11s 25ms/step - loss: 0.1437 - accuracy: 0.9819 - val_loss: 0.9354 - val_accuracy: 0.8252\n",
            "Epoch 52/100\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.1400 - accuracy: 0.9838 - val_loss: 0.9529 - val_accuracy: 0.8227\n",
            "Epoch 53/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1409 - accuracy: 0.9827 - val_loss: 0.9484 - val_accuracy: 0.8236\n",
            "Epoch 54/100\n",
            "469/469 [==============================] - 11s 25ms/step - loss: 0.1382 - accuracy: 0.9839 - val_loss: 0.9620 - val_accuracy: 0.8228\n",
            "Epoch 55/100\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.1355 - accuracy: 0.9855 - val_loss: 0.9874 - val_accuracy: 0.8205\n",
            "Epoch 56/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1346 - accuracy: 0.9849 - val_loss: 0.9704 - val_accuracy: 0.8259\n",
            "Epoch 57/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1354 - accuracy: 0.9853 - val_loss: 0.9687 - val_accuracy: 0.8246\n",
            "Epoch 58/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1327 - accuracy: 0.9860 - val_loss: 0.9950 - val_accuracy: 0.8219\n",
            "Epoch 59/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1327 - accuracy: 0.9859 - val_loss: 0.9921 - val_accuracy: 0.8245\n",
            "Epoch 60/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1307 - accuracy: 0.9865 - val_loss: 0.9765 - val_accuracy: 0.8222\n",
            "Epoch 61/100\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.1302 - accuracy: 0.9864 - val_loss: 0.9645 - val_accuracy: 0.8239\n",
            "Epoch 62/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1301 - accuracy: 0.9867 - val_loss: 0.9723 - val_accuracy: 0.8264\n",
            "Epoch 63/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1272 - accuracy: 0.9876 - val_loss: 0.9998 - val_accuracy: 0.8207\n",
            "Epoch 64/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1256 - accuracy: 0.9880 - val_loss: 0.9905 - val_accuracy: 0.8249\n",
            "Epoch 65/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1258 - accuracy: 0.9878 - val_loss: 0.9798 - val_accuracy: 0.8254\n",
            "Epoch 66/100\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.1255 - accuracy: 0.9882 - val_loss: 1.0005 - val_accuracy: 0.8259\n",
            "Epoch 67/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1259 - accuracy: 0.9882 - val_loss: 1.0066 - val_accuracy: 0.8256\n",
            "Epoch 68/100\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.1241 - accuracy: 0.9888 - val_loss: 0.9827 - val_accuracy: 0.8286\n",
            "Epoch 69/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1251 - accuracy: 0.9883 - val_loss: 1.0045 - val_accuracy: 0.8261\n",
            "Epoch 70/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1262 - accuracy: 0.9883 - val_loss: 0.9973 - val_accuracy: 0.8263\n",
            "Epoch 71/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1237 - accuracy: 0.9885 - val_loss: 1.0178 - val_accuracy: 0.8250\n",
            "Epoch 72/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1238 - accuracy: 0.9892 - val_loss: 1.0073 - val_accuracy: 0.8252\n",
            "Epoch 73/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1217 - accuracy: 0.9897 - val_loss: 1.0062 - val_accuracy: 0.8285\n",
            "Epoch 74/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1221 - accuracy: 0.9893 - val_loss: 1.0251 - val_accuracy: 0.8243\n",
            "Epoch 75/100\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.1210 - accuracy: 0.9898 - val_loss: 1.0311 - val_accuracy: 0.8251\n",
            "Epoch 76/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1214 - accuracy: 0.9895 - val_loss: 1.0281 - val_accuracy: 0.8254\n",
            "Epoch 77/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1196 - accuracy: 0.9901 - val_loss: 1.0106 - val_accuracy: 0.8265\n",
            "Epoch 78/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1189 - accuracy: 0.9906 - val_loss: 0.9997 - val_accuracy: 0.8284\n",
            "Epoch 79/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1192 - accuracy: 0.9900 - val_loss: 1.0193 - val_accuracy: 0.8246\n",
            "Epoch 80/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1189 - accuracy: 0.9904 - val_loss: 1.0248 - val_accuracy: 0.8276\n",
            "Epoch 81/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1187 - accuracy: 0.9904 - val_loss: 1.0168 - val_accuracy: 0.8267\n",
            "Epoch 82/100\n",
            "469/469 [==============================] - 11s 24ms/step - loss: 0.1165 - accuracy: 0.9909 - val_loss: 1.0221 - val_accuracy: 0.8273\n",
            "Epoch 83/100\n",
            "469/469 [==============================] - 11s 25ms/step - loss: 0.1165 - accuracy: 0.9914 - val_loss: 1.0144 - val_accuracy: 0.8305\n",
            "Epoch 84/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1161 - accuracy: 0.9913 - val_loss: 1.0374 - val_accuracy: 0.8246\n",
            "Epoch 85/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1170 - accuracy: 0.9910 - val_loss: 1.0117 - val_accuracy: 0.8284\n",
            "Epoch 86/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1140 - accuracy: 0.9921 - val_loss: 1.0328 - val_accuracy: 0.8291\n",
            "Epoch 87/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1139 - accuracy: 0.9922 - val_loss: 1.0252 - val_accuracy: 0.8326\n",
            "Epoch 88/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1145 - accuracy: 0.9920 - val_loss: 1.0426 - val_accuracy: 0.8267\n",
            "Epoch 89/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1151 - accuracy: 0.9914 - val_loss: 1.0465 - val_accuracy: 0.8272\n",
            "Epoch 90/100\n",
            "469/469 [==============================] - 11s 25ms/step - loss: 0.1146 - accuracy: 0.9919 - val_loss: 1.0385 - val_accuracy: 0.8295\n",
            "Epoch 91/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1151 - accuracy: 0.9912 - val_loss: 1.0333 - val_accuracy: 0.8278\n",
            "Epoch 92/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1129 - accuracy: 0.9922 - val_loss: 1.0343 - val_accuracy: 0.8310\n",
            "Epoch 93/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1127 - accuracy: 0.9922 - val_loss: 1.0455 - val_accuracy: 0.8273\n",
            "Epoch 94/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1129 - accuracy: 0.9925 - val_loss: 1.0340 - val_accuracy: 0.8266\n",
            "Epoch 95/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1115 - accuracy: 0.9932 - val_loss: 1.0526 - val_accuracy: 0.8247\n",
            "Epoch 96/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1119 - accuracy: 0.9926 - val_loss: 1.0431 - val_accuracy: 0.8292\n",
            "Epoch 97/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1111 - accuracy: 0.9932 - val_loss: 1.0410 - val_accuracy: 0.8301\n",
            "Epoch 98/100\n",
            "469/469 [==============================] - 11s 25ms/step - loss: 0.1100 - accuracy: 0.9931 - val_loss: 1.0418 - val_accuracy: 0.8285\n",
            "Epoch 99/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1116 - accuracy: 0.9927 - val_loss: 1.0344 - val_accuracy: 0.8304\n",
            "Epoch 100/100\n",
            "469/469 [==============================] - 12s 25ms/step - loss: 0.1103 - accuracy: 0.9930 - val_loss: 1.0498 - val_accuracy: 0.8283\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-d71795407b16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;31m# history = model.fit(x_train, y_train, epochs=epochs, batch_size=128, verbose=1, validation_data=(testX, testY), shuffle=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_cutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_cutoff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0msummarize_diagnostics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'summarize_diagnostics' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train))"
      ],
      "metadata": {
        "id": "qEw_Mmxgi0HJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfcce5cd-387f-42ca-f76d-ae1b2ef87dc9"
      },
      "id": "qEw_Mmxgi0HJ",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Copy of keras tuner for hyperparameters and model layers of the baseline.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}